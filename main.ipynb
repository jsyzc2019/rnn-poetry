{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding = utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from data import get_data\n",
    "from config import Config\n",
    "from model import PoetryModel\n",
    "from torch import nn, Tensor\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "# 可视化相关\n",
    "from utils import Visualizer\n",
    "import tqdm\n",
    "from torchnet import meter\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Config()\n",
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.use_gpu and (not t.cuda.is_available()) :\n",
    "    opt.use_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(opt, k, v)\n",
    "    \n",
    "    vis = Visualizer(env=opt.env)\n",
    "    \n",
    "    # 获取数据\n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "    data = t.from_numpy(data)\n",
    "    dataloader = t.utils.data.DataLoader(data, batch_size=opt.batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # 定义model\n",
    "    model = PoetryModel(len(word2ix), opt.embedding_dim, opt.hidden_dim)\n",
    "    # 优化器\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=opt.lr)\n",
    "    # Loss Function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 使用预训练的模型，为了可持续训练\n",
    "    if opt.model_path and os.path.exists(opt.model_path):\n",
    "        model.load_state_dict(t.load(opt.model_path))\n",
    "    \n",
    "    # GPU related\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "    \n",
    "    # loss 计量器\n",
    "    loss_meter = meter.AverageValueMeter()\n",
    "    \n",
    "    # for loop\n",
    "    for epoch in range(opt.epoch):\n",
    "        loss_meter.reset()\n",
    "        \n",
    "        # for : batching dataset\n",
    "        for i, data_ in tqdm.tqdm(enumerate(dataloader)):\n",
    "            \n",
    "            # 训练\n",
    "            # data_ \n",
    "            # size: [128, 125]  每次取128行，每行一首诗，长度为125\n",
    "            # type: Tensor\n",
    "            # dtype: torch.int32 应该转成long\n",
    "            \n",
    "            # 这行代码信息量很大：\n",
    "            # 第一步：int32 to long\n",
    "            # 第二步：将行列互换，为了并行计算的需要\n",
    "            # 第三步：将数据放置在连续内存里，避免后续有些操作报错\n",
    "            data_ = data_.long().transpose(0, 1).contiguous()\n",
    "            \n",
    "            # GPU related\n",
    "            if opt.use_gpu:\n",
    "                data_ = data_.to(device)\n",
    "            \n",
    "            # 到这里 data_.dtype又变成了torch.int64\n",
    "            # print(data_.dtype)\n",
    "            \n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 错位训练，很容易理解\n",
    "            # 把前n-1行作为input，把后n-1行作为target  :  model的输入\n",
    "            # 这么做还是为了并行计算的需要\n",
    "            # input_ 加下划线是为了和built_in function input区分开\n",
    "            input_, target = data_[:-1, :], data_[1:, :]\n",
    "            \n",
    "            # model的返回值 output和hidden\n",
    "            # 这里hidden没什么用\n",
    "            output, _ = model(input_)\n",
    "            \n",
    "            # 计算loss\n",
    "            target = target.view(-1)\n",
    "            \n",
    "            # 新的target.size() [15872]  124 * 128 = 15872\n",
    "            # output.size()  [15872, 8293] 8293 是词汇量的大小\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # optimizer梯度下降更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_meter.add(loss.data[0])\n",
    "\n",
    "            # 可视化\n",
    "            if (1 + i) % opt.plot_every == 0:\n",
    "\n",
    "                if os.path.exists(opt.debug_file):\n",
    "                    ipdb.set_trace()\n",
    "\n",
    "                vis.plot('loss', loss_meter.value()[0])\n",
    "\n",
    "                # 诗歌原文\n",
    "                poetrys = [[ix2word[_word.item()] for _word in data_[:, _iii]]\n",
    "                           for _iii in range(data_.size(1))][:16]\n",
    "                vis.text('</br>'.join([''.join(poetry) for poetry in poetrys]), win=u'origin_poem')\n",
    "\n",
    "                gen_poetries = []\n",
    "                # 分别以这几个字作为诗歌的第一个字，生成8首诗\n",
    "                for word in list(u'春江花月夜凉如水'):\n",
    "                    gen_poetry = ''.join(generate(model, word, ix2word, word2ix))\n",
    "                    gen_poetries.append(gen_poetry)\n",
    "                vis.text('</br>'.join([''.join(poetry) for poetry in gen_poetries]), win=u'gen_poem')\n",
    "        # 迭代一次epoch，保存一下模型\n",
    "        t.save(model.state_dict(), '%s_%s.pth' % (opt.model_prefix, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成诗歌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_words, ix2word, word2ix, prefix_words=None):\n",
    "    \"\"\"\n",
    "    给定几个词，根据这几个词接着生成一首完整的诗歌\n",
    "    start_words：u'春江潮水连海平'\n",
    "    比如start_words 为 春江潮水连海平，可以生成：\n",
    "    prefix_words: 用来控制意境\n",
    "\n",
    "    \"\"\"\n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 手动设置第一个词为<START>\n",
    "    # 之所以把size变成[1, 1]是因为model的输入就是二维的\n",
    "    input_ = Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    if opt.use_gpu:\n",
    "        input_ = input_.to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    # 控制意境\n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input_, hidden)\n",
    "            input_ = input_.data.new([word2ix[word]]).view(1, 1)\n",
    "    \n",
    "    # 开始逐词生成诗歌\n",
    "    for i in range(opt.max_gen_len):\n",
    "        output, hidden = model(input_, hidden)\n",
    "        \n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input_ = (input_.data.new([word2ix[w]])).view(1, 1)\n",
    "        else:\n",
    "            # 概率最大的那个词的索引\n",
    "            # output: size [1, 8293] .data size [1, 8293]\n",
    "            # output.data[0] 取第一行 type: torch.Tensor\n",
    "            \n",
    "            # 这行代码信息量很大，首先data是一个二维数组，size [1, 8293]\n",
    "            # data[0]取第一行\n",
    "            # .top(1) 一维数组，取出最大的那个，返回值是个tuple (num, index)\n",
    "            # topk(1)[1][0] 取出index，这个index是个list，取出第一个\n",
    "            # item() 获取Python数值\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            # 新预测出现的词，加入进去\n",
    "            results.append(w)\n",
    "            input_ = (input_.data.new([top_index])).view(1, 1)\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_acrostic(model, start_words, ix2word, word2ix, prefix_words=None):\n",
    "    \"\"\"\n",
    "    生成藏头诗\n",
    "    start_words : u'深度学习'\n",
    "    生成：\n",
    "    深木通中岳，青苔半日脂。\n",
    "    度山分地险，逆浪到南巴。\n",
    "    学道兵犹毒，当时燕不移。\n",
    "    习根通古岸，开镜出清羸。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 手动设置第一个词为<START>\n",
    "    # 之所以把size变成[1, 1]是因为model的输入就是二维的\n",
    "    input_ = Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    if opt.use_gpu:\n",
    "        input_ = input_.to(device)\n",
    "    hidden = None\n",
    "    \n",
    "    index = 0\n",
    "    pre_word = '<START>'\n",
    "    \n",
    "    # 控制意境\n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input_, hidden)\n",
    "            input_ = input_.data.new([word2ix[word]]).view(1, 1)\n",
    "    \n",
    "    # 开始逐词生成诗歌\n",
    "    for i in range(opt.max_gen_len):\n",
    "        output, hidden = model(input_, hidden)\n",
    "        top_index = output.data[0].topk(1)[1][0].item()\n",
    "        w = ix2word[top_index]\n",
    "        \n",
    "        # 如果遇到这些符号，就把藏头词送进去\n",
    "        if pre_word in [u'。', u'！', '<START>']:\n",
    "            if index == start_word_len:\n",
    "                break\n",
    "            else:\n",
    "                w = start_words[index]\n",
    "                index += 1\n",
    "                input_ = (input_.data.new([word2ix[w]])).view(1, 1)\n",
    "        else:\n",
    "            # 否则的话，就把上次的输出作为输入送进去\n",
    "            input_ = (input_.data.new([word2ix[w]])).view(1, 1)\n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(**kwargs):\n",
    "    \"\"\"\n",
    "    gen 提供命令行接口\n",
    "    \"\"\"\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(opt, k, v)\n",
    "    \n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "    model = PoetryModel(len(word2ix), opt.embedding_dim, opt.hidden_dim)\n",
    "    # todo : what is it???\n",
    "    map_location = lambda s, l: s\n",
    "    state_dict = t.load(opt.model_path, map_location=map_location)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    if opt.use_gpu:\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # 默认Python 3.X\n",
    "    if opt.start_words.isprintable:\n",
    "        start_words = opt.start_words\n",
    "        prefix_words = opt.prefix_words\n",
    "    else:\n",
    "        # todo : what is it???\n",
    "        start_words = opt.start_words.encode('ascii', 'surrogateescape').decode('utf8')\n",
    "        prefix_words = opt.prefix_words.encode('ascii', 'surrogateescape').decode('utf8') if opt.prefix_words else None\n",
    "    \n",
    "    # 半角替换成全角\n",
    "    start_words = start_words.replace(',', u'，').replace('.', u'。').replace('?', u'？')\n",
    "    \n",
    "    # 选择合适的生成函数\n",
    "    gen_poetry = gen_acrostic if opt.acrostic else generate\n",
    "    result = gen_poetry(model, start_words, ix2word, word2ix, prefix_words)\n",
    "    print(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
